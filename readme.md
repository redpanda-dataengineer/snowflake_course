1. Построение pipeline на snowflake
    - Функции и методы Snowflake для построения pipeline
        - Snowpipe сервис непрерывной загрузки данных , который позволяет загружать данные п
2. Мониторинг процессов pipeline
3. Обеспечение качества данных
4. Функции snowflake



Описание проектов
1. Загрузка данных из внешних хранилищ (AWS S3) через snowpipe, который автоматически обнаруживает новые файлы и загружает их в snowflake (в каком случае лучше использовать snowpipe а в каком ручной copy into)
2. Хранение и организация данных
    - В таблицах - для хранения структурированных данных. Очень удобно использовать в snowflake так как snowflake автоматически управляет сжатием, оптимизацией и индексацией
    - Виртуальные склады: вычислительные русерсы в snowflake, которые используются для выполнения запросов и операций в данными. Для pipeline-ов важно выбрать подходящий размер склада, чтобы обеспечаить достаточную производительность и управлемость затратамию Разьичные пайпланы могут использовать разные склады.
    - Snowflake Data Cloud: возможность обмена данными с другими аккауентами snowflake через безопасные публикации данных

3. Преобразование данных
    - SQL: ядро всех преобразований в snowflake.
    - SQL UDFs: пользовательские функции написанные на sql
    - Stored Procedures: хранимые процедуры, написанные на python позволяет выполнять последовательности sql-операций, управлять транзакциями, обрабатывать ошибки и включать управляющую длгику в пайплан. Они идеально подходя для оркестрации более сложных процессов преобразования.
    - Views: виртуальные таблицы, основанные на результате SQL- запроса. Используются для урощения сложных запросов, скрытия базовой сложности данных, применения логики преобразования "на летк" и обеспечения безопасности данных.
        - Материализованные представляения: заранее вычисляют и кжшируют резльтаты запроса, что значительно ускоряет выполнение запросов к часто используемым данным. Snowglake автоматически обновляет материализованные представления при изменении збазовых данных. Иденлально подходят для агрегаций и аналитических слоев.

4: Потребление данных
    - Views, Materialized Views: Как упомянутовыше, они также используются для предоставляения очищеннызх и преобразованных данных анатилитикам и BI-инструментам.
    - Snowflake COnnector для различных инструментов: Snowflake предлагает коннекторы для популярных BI-инструментовб ETL инструментов и языков программирования

5. Автоматизация и мониторинг
    - Tasks: позволются планировать выполнение SQL-операций или зранимых процедур по расписанию. Задачи могут быть независимыми или формироваться DAGs (Directed Acyclic Graphs) для последовательного выполнения нескольких шагов пайплайна, при этом последюущие задачи запускаются только после успешного завершения прудыдщих.
    - Streams: отслеживают изменения данных (вставки, обновления, удаления ) в исходных таблицах. Потоки позволяются выполнять инкрементальные обновления, обрабюатывая только новые или измененные данные, что значительно повышает эффективость пайплайнов.
    - Change Data Capture (CDC): В сочетании со Streams< snowflake позволяет реализовать CDC-паттерны для эффективной синхронизации данных между системами.
    - Snowsight : встроенный веб-интерфейс snowflake предоставляет инструменты для молниторинга выполнения запросов, истории загрузок Snowpipe, использования склада и общей производительности системы
    - Information Schema Views: системные представления, содержащие метаданные о базе данных, такие как история запросов, использование склада, статус задач и потоков. Полезны для программного мониторинга и аудита.
    - External Tools (Airflow, dbt): для более сложныхз пайпланой и оркестарции многие пользователи интегрируют snowflake с внешними инструментами, такими как Airfow для плниарования и dbt для управления преобразованиями данных на основе SQL и контроля вресий.

Простой ETL pipeline

1. Загрузка : логи из S3 автоматически загружаются в "сырую" таблицу RAW_LOGS и использованием snowpipe
2. Поток изменений: создается Stream на таблицу RAW_LOGS для отслеживания новых записей
3. Преобразование Создается Task, который запускается, когда поток RAW_LOGS_STRAM не пуст. Этот Task выполняет INSERT INTO ANALYTICS_LOGS SELECT ... FROM RAW_LOGS_STRAM, очищая, парся и обогащая данные логов, а затем сохраняя их в таблицу ANALYTICS_LOGS
4. Анализ Аналитики и BI-инструменты запрашивают данные из ANALYTICS_LOGS или из VIews, построенных поверх нее, возможно, с использованием Materialized Views для часто используесмых агрегаций



Наименование проекта: Перенос данных из API источника и локальной БД metabase в snowflake

Что было: 
- Airflow на сервере Metabase
- Сложные python скрипты для обработки, валидации и трансформации данных
- Данные в виде json файлов из API которые python скриптами добавлялись в базу данных metabase. 
    - Данные являлись оперативными и требуется загружать обновления с интервалом максимум раз в 10 минут
- Данные из локальной БД mysql который python скриптами добавлялись в базу данных metabase
    - Данные не являются оперативными и можно загружать раз в день

Задача
- Перенести данные на snowflake (на базе s3). 
- Снизить нагрузку на сервер metabase

Что сделанно и что использовали:
1. Сбор данных

Задача: Получить данные из API (частые изменения) и из локальной базы данных (ежедневный дамп за предыдущий день)
- Для API
    - Использовали легковесный скрипт на python который раз в 5 минут вызывал api стороннего сервиса. 
    - Полученный данные сохраняли в виде небольших json файлов на S3. 
    - Чтение данных с S3 производили через snowpipe которая автоматически обнаруживала новые файлы и загружала данные в сырую таблицу snowflake.
- Для БД
    - Был написан скрипт по извлечению данных из локальной базы данных в файл CSV. 
    - Далее файл загружался на S3 через boto3.
    - На стороне snowflake создали task с ежеденевным запуском и командой COPY INTO в сырую таблицу новых данных за предыдущий день
2. Хранение и оркестрация

Задача: Организовать сырые данные и настроить автоматизацию их обработки в Snowflake
- Определяем вирутальные склады для разграничения вычислительных мощностей под определенные операции
    - X-SMALL для API Snowpipe
    - LARGE для Tasks
    - MEDIUM для запросов из Metabase
- Streams на сырых данных API: который отслеживает непрерывные изменения в данных из API, что позволяет нам запускать трансформации только в случае если есть новые данные из API
- Tasks:
    - Ежедневная задача для COPY INTO данных из локальной БД
    - Задача обработки новых данных из API при наличии новых данных в STREAM
    - Задача обработки новых данных из локальной БД которая запускается после отработки задачи загрузки сырых данных из локальной БД
    - Задача обновления данных на витрине данных после отработки процесса обработки сырых данных на API или из локальной БД
3. Преобразование данных

Задача: Очистить, обогатить и агрегировать сырые данные в удобный для аналитики формат.
- Формируем промежуточные таблицы (Staging)
    - Для обработки данных из API в Task-е используем Snowpark (Python Stored Procedure) для парсинга JSON, очистки и валидации данных. Процедура считывает сырые данные очищает, валидирует и записывает в слой Staging.
    - Для обработки данных из локальной БД в Task-е также используем Snowpark для проведения дедупликации и нормализации. Процедура считывает сырые данные трансформирует и записывает в слой Staging (что такое дедупликация и нормализация)
- Формируем витрины данных
    - Используем Views Snowflake как модели данных для Metabase
    - Внутри Metabase создаем questions для панелей и выводим результат
4. Потребление данных
- Визуализация в Metabase: Подключаем Metabase напрямую к аккаунту Snowflake, указав имя склада

Результат:

Таким образом снизили нагрузку на сервер Metabase:
1. Замены Airflow на сервере Metabase на Snowflake Tasks и Streams
2. Упрощение pipeline без использования SQLAlchemy и замены скриптов на леговесные по сохранение и передаче файлов в S3
3. Устранили необходимость в локальной БД для metabase

Упростили инфраструктуру, обеспечили высокую автоматизацию, масштабируемость и производительность с использованием большинства возможностей Snowflake.

Добрый день
Опыт работы с Snowflake в качестве централизованной платформы для обработки и анализа данных.
- Разрабатывал и внедрял ETL/ELT pipeline, используя нативные возможности Snowflake
    - Snowpipe для потоковой загрузки данных из внешних API
    - Tasks для оркестрации и планирования ежедненвых преобразований данных из локальных источников
    - Streams для инкрементальной обработки изменений
- Работал с различными типами данных
    - Структурированные
    - Полуструктурированные JSON
- Создавал витрины данных с помощью
    - Materialized Views
    - Оптимизированных таблиц для потребления BI-инструментами
- Владею навыками оптимизации производительности и управления ресурсами с использованием виртуальныз складов.

Технологии:
- Snowpipe
- Snowflake Tasks
- Snowflake Streams (CDC)
- Stored Procedures (Python, SQL)
- Snowpark
- External Stage (S3)
- ETL/ELT pipelines
- Materialized Views
- Views
- Virtual Warehouses
- Обработка JSON-данных
- Оркестрация данных